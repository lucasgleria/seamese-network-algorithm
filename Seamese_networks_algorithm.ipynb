{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasgleria/seamese-network-algorithm/blob/main/Seamese_networks_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README - Projeto de Busca por Similaridade de Imagens com Triplet Loss no MNIST\n",
        "\n",
        "Este projeto explora a implementação de um sistema de busca por similaridade de imagens utilizando redes neurais e a função de perda **Triplet Loss**. O dataset utilizado para os testes e demonstrações é o **MNIST**, composto por dígitos manuscritos.\n",
        "\n",
        "A principal ideia é treinar uma rede neural para aprender a gerar **embeddings** (vetores de características) para imagens, de modo que imagens similares tenham embeddings próximos no espaço vetorial, e imagens diferentes tenham embeddings distantes. Isso permite que, dada uma imagem de \"consulta\", possamos encontrar outras imagens \"semelhantes\" em um banco de dados.\n",
        "\n",
        "## Fase de testes\n",
        "\n",
        "A **Fase de Testes** é dedicada à configuração do ambiente, preparação dos dados e construção e treinamento do modelo. Aqui, você encontrará os scripts e a lógica para:\n",
        "\n",
        "### Implementando o ambiente e configurações iniciais\n",
        "\n",
        "Nesta seção, o ambiente do Google Colab é preparado. Isso envolve a **instalação de bibliotecas essenciais** como `segmentation-models-pytorch` (que inclui dependências para modelos de visão), `albumentations` para aumento de dados e `opencv-contrib-python` para processamento de imagens. Além disso, são definidas **configurações globais** cruciais para o treinamento, como o tamanho do `BATCH_SIZE`, a `Learning Rate (LR)`, o número de `EPOCHS` (épocas de treinamento) e a **seleção do dispositivo de processamento** (`cuda` se houver GPU disponível, `cpu` caso contrário).\n",
        "\n",
        "---\n",
        "\n",
        "### Carregando e Preparando o Dataset MNIST para Triplets\n",
        "\n",
        "Esta parte do projeto detalha como o dataset **MNIST** é carregado. As imagens passam por **transformações** necessárias, como a conversão para **tensores PyTorch** e a **normalização** de seus valores de pixel. O ponto chave desta seção é a preparação dos dados para a **Triplet Loss**:\n",
        "* É implementada uma função auxiliar (`create_class_indices`) para **agrupar os índices das imagens por classe** (dígito). Essa organização é fundamental para a eficiente seleção de pares **Anchor-Positive** (imagens da mesma classe) e **Anchor-Negative** (imagens de classes diferentes).\n",
        "* A classe **`APN_MNIST_Dataset`** é desenvolvida. Ela é uma subclasse de `torch.utils.data.Dataset` que, a cada requisição, gera um **triplet** composto por uma imagem **Anchor**, uma **Positive** (da mesma classe da Anchor) e uma **Negative** (de uma classe diferente). Além disso, ela garante a **compatibilidade de canais** das imagens (de 1 para 3) para uso com modelos pré-treinados como o EfficientNet.\n",
        "* Ao final, você poderá **visualizar um exemplo de triplet** para confirmar a correta geração das amostras.\n",
        "\n",
        "---\n",
        "\n",
        "### Preparando DataLoaders e Definindo o Modelo\n",
        "\n",
        "Com o dataset pronto, os **`DataLoaders`** são configurados para carregar os dados em **batches** durante o treinamento e validação, permitindo o embaralhamento e a otimização do processo. A arquitetura do modelo (`APN_Model`) é definida, utilizando o **EfficientNet-B0** pré-treinado da biblioteca `timm` como *backbone*. A camada classificadora final do EfficientNet é ajustada para produzir os **vetores de embedding** com o tamanho desejado.\n",
        "\n",
        "---\n",
        "\n",
        "### Funções de Treinamento, Avaliação e Loop Principal\n",
        "\n",
        "Esta seção apresenta as funções **`train_fn`** e **`eval_fn`**. A primeira é responsável por executar um passo de treinamento em uma época, incluindo o cálculo da **Triplet Loss**, a retropropagação (backpropagation) e a atualização dos pesos do modelo usando o otimizador **Adam**. A `eval_fn` avalia o desempenho do modelo no conjunto de validação, sem atualização de pesos. O **loop de treinamento** orquestra essas funções por um número definido de épocas, monitora a perda de validação e **salva os pesos do modelo** que obtiver o melhor desempenho.\n",
        "\n",
        "---\n",
        "\n",
        "### Inferência e Busca por Similaridade\n",
        "\n",
        "Após o treinamento, o modelo é utilizado para **inferência**. Uma função **`get_mnist_encodings`** é implementada para gerar os embeddings de um subconjunto do dataset de teste, criando um banco de dados de vetores de características. As funções **`euclidean_dist`** (para calcular a distância entre embeddings) e **`plot_closest_mnist_imgs`** (para visualizar os resultados) são desenvolvidas. Finalmente, um **exemplo prático de busca** é demonstrado: uma imagem de consulta é selecionada, seu embedding é gerado e as imagens mais similares no banco de dados são identificadas e exibidas com base na proximidade de seus embeddings."
      ],
      "metadata": {
        "id": "Ukt_xl-Vv2Un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalações\n"
      ],
      "metadata": {
        "id": "xp5NliOJzRSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instale as bibliotecas essenciais para o projeto.\n",
        "- **'segmentation-models-pytorch'** é uma biblioteca poderosa para tarefas de segmentação.\n",
        "- **'albumentations'** é utilizada para aumento de dados (data augmentation) em tempo real, o que ajuda a melhorar a robustez e generalização do modelo.\n",
        "- **'opencv-contrib-python'** é fundamental para operações de processamento de imagem, e a atualização garante que tenhamos as funcionalidades mais recentes."
      ],
      "metadata": {
        "id": "EGZkEJdyxshy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation-models-pytorch\n",
        "!pip install -U git+https://github.com/albumentations-team/albumentations\n",
        "!pip install --upgrade opencv-contrib-python"
      ],
      "metadata": {
        "id": "ng5kt7rFN2VZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd9e4d6-7df2-41be-bbf2-4a9affba7cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (11.2.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.5.3)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (1.0.15)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->segmentation-models-pytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, segmentation-models-pytorch\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 segmentation-models-pytorch-0.5.0\n",
            "Collecting git+https://github.com/albumentations-team/albumentations\n",
            "  Cloning https://github.com/albumentations-team/albumentations to /tmp/pip-req-build-eqr_hgvl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/albumentations-team/albumentations /tmp/pip-req-build-eqr_hgvl\n",
            "  Resolved https://github.com/albumentations-team/albumentations to commit 19ecdd017458b551db4fb5da167bc6a97d98a298\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.8) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.8) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.8) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.8) (2.11.5)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.8) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations==2.0.8) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations==2.0.8) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations==2.0.8) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.8) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.8) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations==2.0.8) (0.4.1)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-contrib-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importações\n",
        "### Importe as bibliotecas necessárias para o desenvolvimento do modelo.\n"
      ],
      "metadata": {
        "id": "P1ORhVwGzw_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Biblioteca principal do PyTorch para construção e treinamento de redes neurais.\n",
        "from torchvision import datasets, transforms # 'datasets' para carregar conjuntos de dados padrão, 'transforms' para pré-processamento de imagens.\n",
        "from torch.utils.data import Dataset, DataLoader # Ferramentas para criar e gerenciar conjuntos de dados e carregadores de dados personalizados.\n",
        "import numpy as np # Para operações numéricas, especialmente com arrays.\n",
        "import pandas as pd # Para manipulação e análise de dados, útil para lidar com metadados ou rótulos.\n",
        "from PIL import Image # Pillow, essencial para abrir, manipular e salvar imagens.\n",
        "import matplotlib.pyplot as plt # Para visualização de dados e gráficos.\n",
        "from tqdm import tqdm # Para exibir barras de progresso durante iterações, útil para acompanhar o treinamento.\n",
        "import random # Para gerar números aleatórios, usado em várias partes do código, como na divisão de dados."
      ],
      "metadata": {
        "id": "o1fXz7dcz32H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configurações\n",
        "### Defina parâmetros importantes que serão usados em todo o projeto."
      ],
      "metadata": {
        "id": "9QlKyfFzz5JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32 # Define o número de amostras de treinamento processadas antes que os pesos do modelo sejam atualizados.\n",
        "LR = 0.001 # Taxa de aprendizado (Learning Rate), um hiperparâmetro crucial que controla o tamanho dos passos durante a otimização.\n",
        "EPOCHS = 5 # Número de vezes que o conjunto de dados inteiro será passado para a rede neural durante o treinamento.\n",
        "             # Reduzido para '5' para testes rápidos e para demonstrar o fluxo com o dataset MNIST.\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Verifica se uma GPU (CUDA) está disponível e a usa;\n",
        "                                                        # caso contrário, o treinamento será executado na CPU.\n",
        "print(f\"Usando o dispositivo: {DEVICE}\") # Imprime o dispositivo que será utilizado para o treinamento."
      ],
      "metadata": {
        "id": "rHh01Iguz62S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando o Dataset MNIST\n",
        "### carregamos o popular dataset MNIST, que consiste em imagens de dígitos manuscritos. Aplicamos transformações essenciais para prepará-lo para o modelo, como a conversão para tensores PyTorch e a normalização. A normalização é crucial para ajudar o modelo a convergir mais rapidamente e ter um desempenho melhor."
      ],
      "metadata": {
        "id": "B-zE1Id7z-iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define as transformações a serem aplicadas nas imagens do MNIST.\n",
        "# 'transforms.ToTensor()' converte as imagens PIL Image (ou NumPy ndarray) para tensores PyTorch.\n",
        "# Ele também escala os valores dos pixels de [0, 255] para [0.0, 1.0].\n",
        "# 'transforms.Normalize((0.1307,), (0.3081,))' normaliza o tensor.\n",
        "# Os valores médios (0.1307) e desvios padrão (0.3081) são os valores padrão para o dataset MNIST,\n",
        "# calculados sobre todo o dataset de treinamento.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalização padrão do MNIST\n",
        "])\n",
        "\n",
        "# Carrega o dataset de treinamento do MNIST.\n",
        "# '../data' especifica o diretório onde o dataset será salvo ou carregado.\n",
        "# 'train=True' indica que estamos carregando o conjunto de treinamento.\n",
        "# 'download=True' permite que o PyTorch baixe o dataset se ele não estiver presente.\n",
        "# 'transform=transform' aplica as transformações definidas acima a cada imagem.\n",
        "mnist_train_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Carrega o dataset de teste do MNIST.\n",
        "# 'train=False' indica que estamos carregando o conjunto de teste.\n",
        "mnist_test_dataset = datasets.MNIST('../data', train=False, transform=transform)\n",
        "\n",
        "# Imprime o tamanho dos datasets carregados para verificação.\n",
        "print(f\"Tamanho do dataset de treino MNIST: {len(mnist_train_dataset)}\")\n",
        "print(f\"Tamanho do dataset de teste MNIST: {len(mnist_test_dataset)}\")"
      ],
      "metadata": {
        "id": "05v6Ca6z0AL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparando os dados para a Geração de Triplets\n",
        "### Para treinar um modelo com Triplet Loss (Perda de Triplet), precisamos de amostras de Anchor (A), Positive (P) e Negative (N). O Anchor e o Positive pertencem à mesma classe, enquanto o Negative pertence a uma classe diferente. Esta seção foca em organizar o dataset para facilitar a criação desses triplets."
      ],
      "metadata": {
        "id": "QudT5N880CpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para agrupar os índices das imagens por classe.\n",
        "# Isso é essencial para selecionar eficientemente as imagens 'Positive' (mesma classe)\n",
        "# e 'Negative' (classe diferente) para cada 'Anchor'.\n",
        "def create_class_indices(dataset):\n",
        "    # Inicializa um dicionário onde cada chave é um dígito (0-9) e o valor é uma lista vazia.\n",
        "    class_indices = {i: [] for i in range(10)}\n",
        "    # Itera sobre o dataset com seus respectivos índices e rótulos.\n",
        "    for i, (_, label) in enumerate(dataset):\n",
        "        # Adiciona o índice da imagem à lista correspondente ao seu rótulo.\n",
        "        class_indices[label].append(i)\n",
        "    return class_indices\n",
        "\n",
        "# Gera os dicionários de índices por classe para os datasets de treino e teste.\n",
        "train_class_indices = create_class_indices(mnist_train_dataset)\n",
        "test_class_indices = create_class_indices(mnist_test_dataset)"
      ],
      "metadata": {
        "id": "IFVDIEwk0HUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando o Dataset ```APN_MNIST_Dataset``` para Triplets\n",
        "### Aqui, definimos uma classe de dataset personalizada que herda de ```torch.utils.data.Dataset```. Esta classe é responsável por gerar os triplets (Anchor, Positive, Negative) sob demanda."
      ],
      "metadata": {
        "id": "_87GU3aA0I5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define uma classe de Dataset personalizada para gerar triplets (Anchor, Positive, Negative).\n",
        "class APN_MNIST_Dataset(Dataset):\n",
        "    def __init__(self, dataset, class_indices):\n",
        "        \"\"\"\n",
        "        Inicializa o dataset APN_MNIST.\n",
        "\n",
        "        Args:\n",
        "            dataset (torch.utils.data.Dataset): O dataset base (e.g., mnist_train_dataset).\n",
        "            class_indices (dict): Um dicionário mapeando labels para listas de índices de imagens,\n",
        "                                  gerado por 'create_class_indices'.\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.class_indices = class_indices\n",
        "        # Cria uma lista plana de todos os rótulos do dataset para seleção eficiente de negativos.\n",
        "        # Embora o class_indices já tenha os rótulos, esta lista pode ser útil para outras lógicas de seleção.\n",
        "        self.labels = [label for _, label in dataset]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Retorna o número total de itens no dataset.\n",
        "        Para cada imagem no dataset original, vamos gerar um triplet.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retorna um triplet (Anchor, Positive, Negative) dado um índice.\n",
        "\n",
        "        Args:\n",
        "            idx (int): O índice da imagem Anchor no dataset original.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Um triplet de tensores de imagem (anchor_img, positive_img, negative_img).\n",
        "        \"\"\"\n",
        "        # --- Anchor (A) ---\n",
        "        # Seleciona a imagem e o rótulo do Anchor com base no índice fornecido.\n",
        "        anchor_img, anchor_label = self.dataset[idx]\n",
        "\n",
        "        # --- Positive (P) ---\n",
        "        # Seleciona outra imagem que pertence à mesma classe do Anchor.\n",
        "        positive_idx = idx\n",
        "        # Garante que a imagem 'Positive' não seja a mesma imagem do 'Anchor'.\n",
        "        while positive_idx == idx:\n",
        "            positive_idx = random.choice(self.class_indices[anchor_label])\n",
        "        positive_img, _ = self.dataset[positive_idx]\n",
        "\n",
        "        # --- Negative (N) ---\n",
        "        # Seleciona uma imagem que pertence a uma classe diferente do Anchor.\n",
        "        negative_label = random.randint(0, 9)\n",
        "        # Garante que o rótulo da imagem 'Negative' seja diferente do rótulo do 'Anchor'.\n",
        "        while negative_label == anchor_label:\n",
        "            negative_label = random.randint(0, 9)\n",
        "        # Seleciona aleatoriamente um índice de imagem da classe 'Negative' escolhida.\n",
        "        negative_idx = random.choice(self.class_indices[negative_label])\n",
        "        negative_img, _ = self.dataset[negative_idx]\n",
        "\n",
        "        # --- Ajuste de Canais para Compatibilidade com EfficientNet ---\n",
        "        # As imagens do MNIST são em escala de cinza (1 canal).\n",
        "        # Muitos modelos pré-treinados, como o EfficientNet, esperam imagens RGB (3 canais).\n",
        "        # Replicamos o único canal 3 vezes para simular uma imagem RGB.\n",
        "        anchor_img = anchor_img.repeat(3, 1, 1)\n",
        "        positive_img = positive_img.repeat(3, 1, 1)\n",
        "        negative_img = negative_img.repeat(3, 1, 1)\n",
        "\n",
        "        return anchor_img, positive_img, negative_img\n",
        "\n",
        "# Instancia os datasets de treino e teste baseados na classe APN_MNIST_Dataset.\n",
        "train_dataset = APN_MNIST_Dataset(mnist_train_dataset, train_class_indices)\n",
        "test_dataset = APN_MNIST_Dataset(mnist_test_dataset, test_class_indices)\n",
        "\n",
        "# Imprime o tamanho dos datasets APN_MNIST gerados.\n",
        "print(f\"Tamanho do dataset de treino APN_MNIST: {len(train_dataset)}\")\n",
        "print(f\"Tamanho do dataset de teste APN_MNIST: {len(test_dataset)}\")\n"
      ],
      "metadata": {
        "id": "98Pk_W0Y0Kp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizando um Exemplo de Triplet\n",
        "### Antes de prosseguir com o treinamento, é fundamental visualizar um exemplo de triplet para confirmar que a lógica de geração está funcionando como esperado. Isso nos ajuda a ter certeza de que as imagens Anchor, Positive e Negative estão sendo selecionadas corretamente."
      ],
      "metadata": {
        "id": "daV0Ecsp0O2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar um exemplo de triplet\n",
        "idx = 0 # Seleciona o primeiro elemento do dataset de treino para visualização.\n",
        "A, P, N = train_dataset[idx] # Obtém o triplet (Anchor, Positive, Negative) no índice 'idx'.\n",
        "\n",
        "# Cria uma figura com 1 linha e 3 colunas para exibir as três imagens do triplet.\n",
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
        "\n",
        "# Exibe a imagem Anchor.\n",
        "# A.permute(1, 2, 0) muda a ordem dos canais de (C, H, W) para (H, W, C) para que o matplotlib possa exibir.\n",
        "# .squeeze() remove dimensões de tamanho 1 (útil se a imagem for em escala de cinza e tiver um canal extra).\n",
        "# .cpu().numpy() move o tensor para a CPU e o converte para um array NumPy.\n",
        "ax1.set_title(f'Anchor (Label: {mnist_train_dataset[idx][1]})') # Define o título com o rótulo original do Anchor.\n",
        "ax1.imshow(A.permute(1, 2, 0).squeeze().cpu().numpy(), cmap='gray') # 'cmap='gray'' para imagens em escala de cinza.\n",
        "\n",
        "# Exibe a imagem Positive.\n",
        "ax2.set_title(f'Positive (Same Label)')\n",
        "ax2.imshow(P.permute(1, 2, 0).squeeze().cpu().numpy(), cmap='gray')\n",
        "\n",
        "# Exibe a imagem Negative.\n",
        "ax3.set_title(f'Negative (Different Label)')\n",
        "ax3.imshow(N.permute(1, 2, 0).squeeze().cpu().numpy(), cmap='gray')\n",
        "\n",
        "plt.show() # Mostra a figura."
      ],
      "metadata": {
        "id": "3RDzej0B0QPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carregando o Dataset em Batches\n",
        "### Para um treinamento eficiente de redes neurais, é comum processar os dados em pequenos lotes (batches). Os ```DataLoaders``` do PyTorch nos ajudam a iterar sobre o dataset em batches, além de permitir embaralhar os dados para evitar que o modelo aprenda a ordem das amostras."
      ],
      "metadata": {
        "id": "mQwR8qKV0TYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar Dataset em batches\n",
        "# Cria um DataLoader para o conjunto de treinamento.\n",
        "# 'batch_size=BATCH_SIZE' define o número de amostras por lote.\n",
        "# 'shuffle=True' embaralha os dados em cada época, o que é crucial para um bom treinamento.\n",
        "trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Cria um DataLoader para o conjunto de validação (teste).\n",
        "# 'shuffle=False' é geralmente usado para o conjunto de validação/teste, pois a ordem não importa.\n",
        "validloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Imprime o número de batches em cada DataLoader.\n",
        "print(f\"\\nNo. de batches em trainloader : {len(trainloader)}\")\n",
        "print(f\"No. de batches em validloader : {len(validloader)}\")\n",
        "\n",
        "# Pega um batch de exemplo para verificar o formato das imagens.\n",
        "for A, P, N in trainloader:\n",
        "    break # Sai do loop após pegar o primeiro batch.\n",
        "print(f\"Formato de um batch de imagem: {A.shape}\") # Imprime o formato do tensor das imagens Anchor (Batch_Size, Canais, Altura, Largura)."
      ],
      "metadata": {
        "id": "02K_A5Ut0VM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Definindo a Arquitetura do Modelo (```APN_Model```)\n",
        "### Aqui, definimos o modelo principal (```APN_Model```) que será usado para gerar os embeddings das imagens. Estamos utilizando o ```EfficientNet-B0``` pré-treinado do timm (```PyTorch Image Models```), que é um modelo eficiente e robusto para tarefas de visão computacional. A camada classificadora final do ```EfficientNet``` é adaptada para produzir um vetor de embedding do tamanho desejado (```emb_size```)."
      ],
      "metadata": {
        "id": "GTNCBhEc0ZqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm # Biblioteca para modelos de imagem pré-treinados, como EfficientNet.\n",
        "import torch.nn.functional as F # Funções comuns de ativação e perda.\n",
        "from torch import nn # Módulo principal do PyTorch para construir redes neurais.\n",
        "\n",
        "# Define a classe do modelo para gerar embeddings APN (Anchor, Positive, Negative).\n",
        "class APN_Model(nn.Module):\n",
        "    def __init__(self, emb_size=512):\n",
        "        \"\"\"\n",
        "        Inicializa o modelo APN.\n",
        "\n",
        "        Args:\n",
        "            emb_size (int): O tamanho do vetor de embedding de saída do modelo.\n",
        "        \"\"\"\n",
        "        super(APN_Model, self).__init__()\n",
        "        # Carrega o modelo EfficientNet-B0 pré-treinado do 'timm'.\n",
        "        # 'pretrained=True' carrega pesos pré-treinados no ImageNet, o que acelera o treinamento.\n",
        "        # O EfficientNet-B0 espera 3 canais de entrada, o que já foi tratado no APN_MNIST_Dataset.\n",
        "        self.efficientnet = timm.create_model('efficientnet_b0', pretrained=True)\n",
        "        # Ajusta a camada classificadora final do EfficientNet para produzir 'emb_size' features.\n",
        "        # Isso transforma a saída do EfficientNet em um vetor de embedding com o tamanho especificado.\n",
        "        self.efficientnet.classifier = nn.Linear(in_features=self.efficientnet.classifier.in_features, out_features=emb_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Passa as imagens pelo modelo para obter seus embeddings.\n",
        "\n",
        "        Args:\n",
        "            images (torch.Tensor): Um tensor de imagens de entrada.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Os vetores de embedding das imagens.\n",
        "        \"\"\"\n",
        "        embeddings = self.efficientnet(images)\n",
        "        return embeddings\n",
        "\n",
        "# Instancia o modelo APN_Model.\n",
        "model = APN_Model()\n",
        "# Move o modelo para o dispositivo de processamento (GPU ou CPU) definido globalmente.\n",
        "model.to(DEVICE)\n",
        "\n",
        "print(f\"Modelo carregado no dispositivo: {DEVICE}\")"
      ],
      "metadata": {
        "id": "YMMjjrrj0ciX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções de Treino e Avaliação e Loop de Treinamento\n",
        "### Esta seção implementa as funções essenciais para o processo de treinamento e validação do modelo. A ```train_fn``` realiza um passo de otimização em cada batch, calculando a perda e atualizando os pesos do modelo. A ```eval_fn``` avalia o desempenho do modelo no conjunto de validação sem atualizar os pesos. O loop de treinamento itera sobre as épocas, chamando essas funções e salvando o melhor modelo com base na perda de validação."
      ],
      "metadata": {
        "id": "V9v4HtSE0f2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Funções de Treino e Avaliação ---\n",
        "\n",
        "def train_fn(model, dataloader, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Função para realizar uma época de treinamento do modelo.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): O modelo a ser treinado.\n",
        "        dataloader (DataLoader): O DataLoader para o conjunto de treinamento.\n",
        "        optimizer (torch.optim.Optimizer): O otimizador.\n",
        "        criterion (nn.Module): A função de perda (TripletMarginLoss).\n",
        "\n",
        "    Returns:\n",
        "        float: A perda média por batch na época de treinamento.\n",
        "    \"\"\"\n",
        "    model.train() # Coloca o modelo em modo de treinamento.\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Itera sobre os batches do dataloader com uma barra de progresso.\n",
        "    for A, P, N in tqdm(dataloader, desc=\"Treinando\"):\n",
        "        # Move os tensores de imagem (Anchor, Positive, Negative) para o dispositivo.\n",
        "        A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
        "\n",
        "        # Passa as imagens pelo modelo para obter seus embeddings.\n",
        "        A_embs = model(A)\n",
        "        P_embs = model(P)\n",
        "        N_embs = model(N)\n",
        "\n",
        "        # Calcula a Triplet Loss.\n",
        "        loss = criterion(A_embs, P_embs, N_embs)\n",
        "\n",
        "        # Zera os gradientes acumulados do otimizador.\n",
        "        optimizer.zero_grad()\n",
        "        # Realiza a retropropagação (backpropagation) para calcular os gradientes.\n",
        "        loss.backward()\n",
        "        # Atualiza os pesos do modelo usando o otimizador.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acumula a perda do batch.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Retorna a perda média por batch para a época.\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def eval_fn(model, dataloader, criterion):\n",
        "    \"\"\"\n",
        "    Função para avaliar o modelo no conjunto de validação.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): O modelo a ser avaliado.\n",
        "        dataloader (DataLoader): O DataLoader para o conjunto de validação.\n",
        "        criterion (nn.Module): A função de perda (TripletMarginLoss).\n",
        "\n",
        "    Returns:\n",
        "        float: A perda média por batch na época de validação.\n",
        "    \"\"\"\n",
        "    model.eval() # Coloca o modelo em modo de avaliação (desativa dropout, batchnorm, etc.).\n",
        "    total_loss = 0.0\n",
        "\n",
        "    # Desativa o cálculo de gradientes para economizar memória e acelerar a inferência.\n",
        "    with torch.no_grad():\n",
        "        # Itera sobre os batches do dataloader de validação.\n",
        "        for A, P, N in tqdm(dataloader, desc=\"Validando\"):\n",
        "            A, P, N = A.to(DEVICE), P.to(DEVICE), N.to(DEVICE)\n",
        "\n",
        "            A_embs = model(A)\n",
        "            P_embs = model(P)\n",
        "            N_embs = model(N)\n",
        "\n",
        "            loss = criterion(A_embs, P_embs, N_embs)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Define a função de perda TripletMarginLoss.\n",
        "# O 'margin' é um hiperparâmetro crucial que define a distância mínima desejada\n",
        "# entre (Anchor, Positive) e (Anchor, Negative).\n",
        "criterion = nn.TripletMarginLoss()\n",
        "# Define o otimizador Adam, que ajusta os pesos do modelo com base nos gradientes.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# --- Loop de Treinamento ---\n",
        "best_valid_loss = np.inf # Inicializa a melhor perda de validação como infinito.\n",
        "\n",
        "print(\"\\nIniciando o treinamento...\")\n",
        "for i in range(EPOCHS): # Itera pelo número de épocas definido.\n",
        "    train_loss = train_fn(model, trainloader, optimizer, criterion) # Treina o modelo por uma época.\n",
        "    valid_loss = eval_fn(model, validloader, criterion) # Avalia o modelo no conjunto de validação.\n",
        "\n",
        "    # Salva o modelo se a perda de validação atual for a melhor encontrada até agora.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(model.state_dict(), 'best_mnist_model.pt') # Salva apenas os pesos do modelo.\n",
        "        best_valid_loss = valid_loss # Atualiza a melhor perda de validação.\n",
        "        print(\"SALVOS_PESOS_SUCESSO\") # Mensagem de sucesso ao salvar.\n",
        "\n",
        "    # Imprime os resultados da época.\n",
        "    print(f\"ÉPOCA: {i+1} Loss Treino: {train_loss:.4f} Loss Validação: {valid_loss:.4f}\")\n",
        "\n",
        "# Carrega os pesos do melhor modelo salvo após o treinamento.\n",
        "model.load_state_dict(torch.load('best_mnist_model.pt'))\n",
        "print(\"\\nMelhor modelo carregado para inferência.\")"
      ],
      "metadata": {
        "id": "bB6vdIV30kgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gerando Embeddings para o Dataset de Teste\n",
        "### Primeiro, definimos uma função para processar um subconjunto do dataset de teste e gerar seus embeddings. Limitamos o número de amostras para uma execução mais rápida e eficiente."
      ],
      "metadata": {
        "id": "NpFoMEev0p8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mnist_encodings(model, dataset, num_samples=5000):\n",
        "    \"\"\"\n",
        "    Gera embeddings para um determinado número de amostras do dataset MNIST.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): O modelo treinado para gerar embeddings.\n",
        "        dataset (torch.utils.data.Dataset): O dataset MNIST (mnist_test_dataset ou mnist_train_dataset).\n",
        "        num_samples (int): O número máximo de amostras para gerar embeddings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Um DataFrame contendo os embeddings e os rótulos correspondentes.\n",
        "    \"\"\"\n",
        "    encodings = []  # Lista para armazenar os vetores de embedding.\n",
        "    labels = []     # Lista para armazenar os rótulos correspondentes.\n",
        "\n",
        "    # Utiliza um DataLoader para processar as imagens em batches, otimizando a geração de embeddings.\n",
        "    dataloader_inference = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model.eval()  # Coloca o modelo em modo de avaliação.\n",
        "    with torch.no_grad():  # Desativa o cálculo de gradientes para otimizar a inferência.\n",
        "        processed_samples = 0\n",
        "        # Itera sobre os batches do DataLoader.\n",
        "        for imgs, lbls in tqdm(dataloader_inference, desc=\"Gerando embeddings MNIST\"):\n",
        "            # Se o número de amostras processadas atingir o limite, interrompe.\n",
        "            if processed_samples >= num_samples:\n",
        "                break\n",
        "\n",
        "            # Replicar canais: MNIST tem 1 canal, EfficientNet espera 3.\n",
        "            # .repeat(1, 3, 1, 1) replica o canal existente 3 vezes.\n",
        "            imgs = imgs.repeat(1, 3, 1, 1).to(DEVICE)\n",
        "            # Passa as imagens pelo modelo para obter os embeddings.\n",
        "            img_encs = model(imgs)\n",
        "            # Adiciona os embeddings processados (convertidos para NumPy na CPU) à lista.\n",
        "            encodings.extend(img_encs.squeeze().cpu().detach().numpy())\n",
        "            # Adiciona os rótulos correspondentes à lista.\n",
        "            labels.extend(lbls.cpu().numpy())\n",
        "            # Incrementa o contador de amostras processadas.\n",
        "            processed_samples += imgs.shape[0]\n",
        "\n",
        "    # Converte as listas de embeddings e rótulos em arrays NumPy.\n",
        "    encodings = np.array(encodings)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Cria um DataFrame Pandas com os embeddings e adiciona uma coluna 'label'.\n",
        "    df_enc_mnist = pd.DataFrame(encodings)\n",
        "    df_enc_mnist['label'] = labels\n",
        "    return df_enc_mnist\n",
        "\n",
        "# Gerar embeddings para uma parte do dataset de teste para inferência.\n",
        "# Usamos 1000 amostras para este exemplo, para agilizar.\n",
        "df_enc_mnist_test = get_mnist_encodings(model, mnist_test_dataset, num_samples=1000)\n",
        "print(\"\\nEmbeddings MNIST gerados (cabeçalho do DataFrame):\")\n",
        "print(df_enc_mnist_test.head())"
      ],
      "metadata": {
        "id": "evOLYx0J0rbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funções Auxiliares para Cálculo de Distância e Visualização\n",
        "### Para encontrar as imagens mais similares, precisamos de uma métrica de distância e uma forma de visualizar os resultados. A distância euclidiana é uma escolha comum para comparar embeddings."
      ],
      "metadata": {
        "id": "_D6dvYWz0v0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Adaptação de euclidean_dist e plot_closest_imgs ---\n",
        "\n",
        "def euclidean_dist(img_enc, ref_enc_arr):\n",
        "    \"\"\"\n",
        "    Calcula a distância euclidiana entre um embedding de consulta e um array de embeddings de referência.\n",
        "\n",
        "    Args:\n",
        "        img_enc (np.ndarray): O embedding da imagem de consulta (1D ou 2D).\n",
        "        ref_enc_arr (np.ndarray): Um array de embeddings de referência (2D).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Um array de distâncias euclidianas.\n",
        "    \"\"\"\n",
        "    # Garante que ambos os arrays são 2D para que a operação de subtração funcione corretamente.\n",
        "    img_enc = img_enc.reshape(1, -1) if img_enc.ndim == 1 else img_enc\n",
        "    ref_enc_arr = ref_enc_arr.reshape(1, -1) if ref_enc_arr.ndim == 1 else ref_enc_arr\n",
        "    # Calcula a norma L2 (distância euclidiana) ao longo do eixo das features.\n",
        "    dist = np.linalg.norm(img_enc - ref_enc_arr, axis=1)\n",
        "    return dist\n",
        "\n",
        "def plot_closest_mnist_imgs(mnist_dataset, df_enc_mnist, query_img_tensor, query_img_label, closest_idx, distance, no_of_closest=5):\n",
        "    \"\"\"\n",
        "    Plota a imagem de consulta e as N imagens mais próximas encontradas.\n",
        "\n",
        "    Args:\n",
        "        mnist_dataset (torch.utils.data.Dataset): O dataset original do MNIST.\n",
        "        df_enc_mnist (pd.DataFrame): DataFrame contendo os embeddings e rótulos das imagens de referência.\n",
        "        query_img_tensor (torch.Tensor): O tensor da imagem de consulta.\n",
        "        query_img_label (int): O rótulo da imagem de consulta.\n",
        "        closest_idx (np.ndarray): Índices das imagens mais próximas no df_enc_mnist (ordenados por distância).\n",
        "        distance (np.ndarray): Array das distâncias correspondentes.\n",
        "        no_of_closest (int): O número de imagens mais próximas a plotar.\n",
        "    \"\"\"\n",
        "    # Cria uma figura com o número de imagens mais próximas + 1 (para a imagem de consulta).\n",
        "    f, axes = plt.subplots(1, no_of_closest + 1, figsize=(15, 5))\n",
        "\n",
        "    # --- Imagem de Consulta ---\n",
        "    axes[0].set_title(f'Consulta: {query_img_label}')\n",
        "    # Converte o tensor para um array NumPy e ajusta as dimensões para exibição.\n",
        "    axes[0].imshow(query_img_tensor.permute(1, 2, 0).squeeze().cpu().numpy(), cmap='gray')\n",
        "    axes[0].axis('off') # Remove os eixos.\n",
        "\n",
        "    # --- Imagens Mais Próximas ---\n",
        "    for i in range(no_of_closest):\n",
        "        idx_in_df = closest_idx[i] # O índice da imagem mais próxima no DataFrame de embeddings.\n",
        "        # Recupera o índice original da imagem no dataset MNIST usando o índice do DataFrame.\n",
        "        original_idx_in_dataset = df_enc_mnist.index[idx_in_df]\n",
        "\n",
        "        # Obtém a imagem e o rótulo do dataset original.\n",
        "        closest_img_tensor, closest_img_label = mnist_dataset[original_idx_in_dataset]\n",
        "\n",
        "        # Reverte a replicação de canais se a imagem tiver 3 canais (para exibição como escala de cinza).\n",
        "        if closest_img_tensor.shape[0] == 3:\n",
        "            closest_img_tensor = closest_img_tensor[0:1, :, :] # Pega apenas o primeiro canal.\n",
        "\n",
        "        # Define o título com a distância e o rótulo da imagem mais próxima.\n",
        "        axes[i+1].set_title(f'Dist: {distance[idx_in_df]:.2f}\\nLabel: {closest_img_label}')\n",
        "        # Exibe a imagem mais próxima.\n",
        "        axes[i+1].imshow(closest_img_tensor.permute(1, 2, 0).squeeze().cpu().numpy(), cmap='gray')\n",
        "        axes[i+1].axis('off') # Remove os eixos.\n",
        "\n",
        "    plt.tight_layout() # Ajusta o layout para evitar sobreposição.\n",
        "    plt.show() # Mostra a figura."
      ],
      "metadata": {
        "id": "JSqpnuHh0xYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exemplo de Inferência e Busca\n",
        "### Finalmente, colocamos tudo junto para realizar um exemplo prático: selecionamos uma imagem aleatória do dataset de teste, geramos seu embedding, calculamos as distâncias para todas as outras imagens no nosso banco de embeddings e visualizamos as mais próximas."
      ],
      "metadata": {
        "id": "niSSkcGm0z0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleciona uma imagem de teste aleatória para ser a imagem de consulta.\n",
        "query_idx = random.randint(0, len(mnist_test_dataset) - 1)\n",
        "query_img, query_label = mnist_test_dataset[query_idx]\n",
        "\n",
        "# Pré-processa a imagem de consulta para que seja compatível com o modelo (3 canais e dimensão de batch).\n",
        "query_img_processed = query_img.repeat(3, 1, 1).to(DEVICE) # Replicar canais e mover para o DEVICE.\n",
        "\n",
        "model.eval() # Coloca o modelo em modo de avaliação.\n",
        "with torch.no_grad(): # Desativa o cálculo de gradientes.\n",
        "    # Gera o embedding da imagem de consulta. .unsqueeze(0) adiciona uma dimensão de batch.\n",
        "    query_enc = model(query_img_processed.unsqueeze(0))\n",
        "    # Move o embedding para a CPU e converte para NumPy.\n",
        "    query_enc = query_enc.detach().cpu().numpy()\n",
        "\n",
        "# Obtém os embeddings e rótulos do DataFrame de referência (todas as colunas exceto 'label').\n",
        "reference_enc_arr = df_enc_mnist_test.iloc[:, :-1].to_numpy()\n",
        "reference_labels = df_enc_mnist_test['label'].to_numpy()\n",
        "\n",
        "# Calcula as distâncias euclidianas entre o embedding de consulta e todos os embeddings de referência.\n",
        "distances = euclidean_dist(query_enc, reference_enc_arr)\n",
        "\n",
        "# Encontra os índices das imagens mais próximas (ordenando as distâncias em ordem crescente).\n",
        "closest_indices = np.argsort(distances)\n",
        "\n",
        "# Plota a imagem de consulta e as 5 imagens mais próximas.\n",
        "plot_closest_mnist_imgs(mnist_test_dataset, df_enc_mnist_test, query_img, query_label, closest_indices, distances, no_of_closest=5)"
      ],
      "metadata": {
        "id": "zIePN_qf03O2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}